[agent]
## files
# comma-delimited list of absolute file paths. If any are a directory, all files in that directory will be loaded.
file_path = /Users/gjn/downloads/cassandra.csv
# regex for file names (does not include the file path)
file_name_regex =

## filters
# define a list of filters to use as field:allowed values|field:allowed values
# for example:
#   message.env:stg,prd|message.status:complete
filters_include =
# as above, but instead define values that are not allowed
# for example:
#   message.env:dev,cde|message.status:draft
filters_exclude =

# raw, rawtail, csv, csvtail, xls, xlsx, json, jsontail, avro, or xml
# *tail formats keep track of the current file & position + completed files in [state] below
data_format = ifexport

## RAW
# if raw data, the regex used to parse the log. It must use named capture groups `(?<name>.*)` that correspond to the *_field config variables below (ie  `(?<timestamp>.*)`,  `(?<host>.*)`,  `(?<device>.*)`,  `(?<etc>.*)`. The raw message will be treated as a field named `_raw`
raw_regex =
# if the log is multiline, the regex that indicates that the line is the start of a new message. this MUST start with ^ (unless blank)
raw_start_regex =

## CSV, XLS, XLSX
# Required - field names, in order as timestamp,field1,field2...
csv_field_names = timestamp,cpu[node1]:1,memory[node1]:2,disk_read[node1]:3,disk_write[node1]:4,network_receive[node1]:5,network_send[node1]:6, cpu[node2]:1,memory[node2]:2,disk_read[node2]:3,disk_write[node2]:4,network_receive[node2]:5,network_send[node2]:6, cpu[node3]:1,memory[node3]:2,disk_read[node3]:3,disk_write[node3]:4,network_receive[node3]:5,network_send[node3]:6, cpu[node4]:1,memory[node4]:2,disk_read[node4]:3,disk_write[node4]:4,network_receive[node4]:5,network_send[node4]:6, cpu[node5]:1,memory[node5]:2,disk_read[node5]:3,disk_write[node5]:4,network_receive[node5]:5,network_send[node5]:6
# a regex string used to delimit "csv" fields (ie the default ,|\t matches a comma or tab character)
csv_field_delimiter = ,|\t

## JSON, AVRO, XML
# for multi-entry messages, define the top-level
#   if messages are [{message1}, {message2}], set top_level = []
#   it's expected that the top level will be a list
# fields in json can be defined as level0.level1.levelN
json_top_level =

## message parsing
# timestamp format, as per python strptime. multiple fields can be formatted together to create the timestamp a la `{date} {time}`, where `date` and `time` are fields that each contain part of the timestamp. If multiple fields could contain the timestamp, a comma-delimited list may be entered (no value of which may use the aforementioned {formatting}), of which the first found will be selected (treating the list as a priority list). ex `timestamp1,timestamp2`.
timestamp_format = epoch
# timezone, as per pytz
timezone =
timestamp_field = timestamp
# if no instance given, the local hostname will be used. Can also use {field} formatting or a priority list.
instance_field = 
device_field =
# multiple fields are separated by commas. a field can be named with the syntax `<name>:<value>` or `<name>:=<value>`, where `<name>` and `<value>` can each be either a literal value (`name:value`) or formatted (`Total Time [{step}]:={timing.end}-{timing.start}`). Use `:=` as a separator to treat `<value>` as a mathematical formula, which must be parseable by `numexpr.evaluate()`.
data_fields =

[insightfinder]
user_name = zinan
license_key = 52a329a52040ed0bceec2c59eb5d1cf102370685
token =
project_name = metric-replay-test
# metric, metricreplay, log, logreplay, incident, incidentreplay, alert, alertreplay, deployment, deploymentreplay
project_type = metricreplay
sampling_interval = 1
run_interval = 10
# what size to limit chunks sent to IF to, as kb
chunk_size_kb = 2048
if_url = https://stg.insightfinder.com
if_http_proxy =
if_https_proxy =

[state]
## do not edit the below fields
current_file =
current_file_offset =
completed_files_st_ino =
